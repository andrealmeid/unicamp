{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFiles(path, data_size=19393):\n",
    "    data = []\n",
    "    for i in range(0, data_size):\n",
    "        with open(path + str(i) + \".txt\", 'r') as f:\n",
    "            data.append(f.read())\n",
    "    return data\n",
    "\n",
    "def getData(stopwords=False, stem=None):\n",
    "    data_size = 19393\n",
    "    data_path = \"data/\"\n",
    "\n",
    "    if not stopwords  and stem == None:\n",
    "        return getFiles(data_path + \"no-stem/original/original\")\n",
    "\n",
    "    if stopwords and stem == None:\n",
    "        return getFiles(data_path + \"no-stem/sw/sw\")\n",
    "\n",
    "    if stem == \"lanc\" and not stopwords:\n",
    "        return getFiles(data_path + \"stem/original/lanc/lanc\")\n",
    "\n",
    "    if stem == \"lanc\" and stopwords:\n",
    "        return getFiles(data_path + \"stem/sw/lanc/lanc\")\n",
    "\n",
    "    if stem == \"porter\" and not stopwords:\n",
    "        return getFiles(data_path + \"stem/original/porter/porter\")\n",
    "\n",
    "    if stem == \"porter\" and stopwords:\n",
    "        return getFiles(data_path + \"stem/sw/porter/porter\")\n",
    "\n",
    "    if stem == \"snow\" and not stopwords:\n",
    "        return getFiles(data_path + \"stem/original/snow/snow\")\n",
    "\n",
    "    if stem == \"snow\" and stopwords:\n",
    "        return getFiles(data_path + \"stem/sw/snow/snow\")\n",
    "\n",
    "    if stem == \"wn\" and not stopwords:\n",
    "        return getFiles(data_path + \"stem/original/wn/wn\")\n",
    "\n",
    "    if stem == \"wn\" and stopwords:\n",
    "        return getFiles(data_path + \"stem/sw/wn/wn\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total terms: 5062336\n",
      "Unique terms: 368042\n",
      "5062336\n",
      "213916, the\n",
      "117481, to\n",
      "105565, of\n",
      " 93844, a\n",
      " 87199, and\n",
      " 64854, is\n",
      " 63686, I\n",
      " 62415, in\n",
      " 59209, that\n",
      " 47701, >\n",
      " 40775, for\n",
      " 34090, you\n",
      " 32783, it\n",
      " 29996, be\n",
      " 29450, have\n",
      "Total terms: 2931680\n",
      "Unique terms: 177684\n",
      "2931680\n",
      " 15270, would\n",
      " 14409, one\n",
      " 13966, writes\n",
      " 11937, article\n",
      "  9871, dont\n",
      "  9665, like\n",
      "  9539, people\n",
      "  9226, it\n",
      "  8717, 1\n",
      "  8698, know\n",
      "  8305, get\n",
      "  7633, think\n",
      "  7323, also\n",
      "  7150, the\n",
      "  6904, 2\n",
      "Total terms: 4876222\n",
      "Unique terms: 176183\n",
      "4876222\n",
      "242865, the\n",
      "126368, a\n",
      "122005, to\n",
      "108943, of\n",
      " 95239, and\n",
      " 81423, in\n",
      " 70648, is\n",
      " 70519, i\n",
      " 64534, that\n",
      " 53288, it\n",
      " 45073, for\n",
      " 42310, you\n",
      " 33122, this\n",
      " 32436, on\n",
      " 31297, be\n",
      "Total terms: 2932597\n",
      "Unique terms: 171448\n",
      "2932597\n",
      " 22125, us\n",
      " 15275, would\n",
      " 14678, on\n",
      " 13737, writes\n",
      " 11805, artic\n",
      " 10473, get\n",
      " 10077, lik\n",
      "  9871, dont\n",
      "  9575, know\n",
      "  9318, it\n",
      "  8717, 1\n",
      "  8600, im\n",
      "  8424, think\n",
      "  8102, the\n",
      "  8017, peopl\n"
     ]
    }
   ],
   "source": [
    "getTop10(getData())\n",
    "getTop10(getData(True))\n",
    "getTop10(getData(False, \"wn\"))\n",
    "getTop10(getData(True, \"lanc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = getData(True)\n",
    "\n",
    "datao = \"\"\n",
    "for d in data:\n",
    "    datao += d\n",
    "    \n",
    "datao = datao.split()\n",
    "vectorizer = TfidfVectorizer()\n",
    "tdm = vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=2, random_state=0, n_jobs=(-1)).fit(tdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(km, open(\"km.txt\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "km = pickle.load( open(\"km.txt\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=2, n_init=10, n_jobs=-1, precompute_distances='auto',\n",
       "    random_state=0, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for i in range(0, 1000):\n",
    "#   print(km.labels_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def getTop10(data):\n",
    "    datao = \"\"\n",
    "    for d in data:\n",
    "        datao += d\n",
    "\n",
    "    datao = datao.split()\n",
    "    print(\"Total terms: {}\".format(len(datao)))\n",
    "    num_unique_terms = len(set(datao))\n",
    "    print(\"Unique terms: {}\".format(num_unique_terms))\n",
    "\n",
    "    print(len(datao))\n",
    "    text = nltk.Text(datao)\n",
    "\n",
    "    fdist1 = nltk.probability.FreqDist(text)\n",
    "    for w in fdist1.most_common(15):\n",
    "        print(\"{:>6}, {}\".format(w[1], w[0]))\n",
    "\n",
    "def printClusterTerms(km, vectorizer):\n",
    "    assigned_cluster = km.labels_\n",
    "    cluster_terms = []\n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(km.n_clusters):\n",
    "        print(\"Cluster {:>2} ({:>4} docs): \".format(i, len([x for x in km.labels_ if x == i])), end='')\n",
    "        cterms = []\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' {}'.format(terms[ind]), end='')\n",
    "            cterms.append(terms[ind])\n",
    "        cluster_terms.append(' '.join(cterms))\n",
    "        print(\"\")\n",
    "        \n",
    "def trainCustomSets(data, k):\n",
    "    getTop10(data)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tdm = vectorizer.fit_transform(data)\n",
    "    km = KMeans(n_clusters=k, random_state=0, n_jobs=(-1)).fit(tdm)\n",
    "    print\n",
    "    printClusterTerms(km, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  0 (13584 docs):  writes would thanks article one know anyone like get email\n",
      "Cluster  1 (5809 docs):  people would one writes god dont article think it us\n"
     ]
    }
   ],
   "source": [
    "def printClusterTerms(km, vectorizer):\n",
    "    assigned_cluster = km.labels_\n",
    "    cluster_terms = []\n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(km.n_clusters):\n",
    "        print(\"Cluster {:>2} ({:>4} docs): \".format(i, len([x for x in km.labels_ if x == i])), end='')\n",
    "        cterms = []\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' {}'.format(terms[ind]), end='')\n",
    "            cterms.append(terms[ind])\n",
    "        cluster_terms.append(' '.join(cterms))\n",
    "        print(\"\")\n",
    "\n",
    "printClusterTerms(km, vectorizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
