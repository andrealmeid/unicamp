{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFiles(path, data_size=19393):\n",
    "    data = []\n",
    "    for i in range(0, data_size):\n",
    "        with open(path + str(i) + \".txt\", 'r') as f:\n",
    "            data.append(f.read())\n",
    "    return data\n",
    "\n",
    "def getData(stopwords=False, stem=None):\n",
    "    data_size = 19393\n",
    "    data_path = \"data/\"\n",
    "\n",
    "    if not stopwords  and stem == None:\n",
    "        return getFiles(data_path + \"no-stem/original/original\")\n",
    "\n",
    "    if stopwords and stem == None:\n",
    "        return getFiles(data_path + \"no-stem/sw/sw\")\n",
    "\n",
    "    if stem == \"lanc\" and not stopwords:\n",
    "        return getFiles(data_path + \"stem/original/lanc/lanc\")\n",
    "\n",
    "    if stem == \"lanc\" and stopwords:\n",
    "        return getFiles(data_path + \"stem/sw/lanc/lanc\")\n",
    "\n",
    "    if stem == \"porter\" and not stopwords:\n",
    "        return getFiles(data_path + \"stem/original/porter/porter\")\n",
    "\n",
    "    if stem == \"porter\" and stopwords:\n",
    "        return getFiles(data_path + \"stem/sw/porter/porter\")\n",
    "\n",
    "    if stem == \"snow\" and not stopwords:\n",
    "        return getFiles(data_path + \"stem/original/snow/snow\")\n",
    "\n",
    "    if stem == \"snow\" and stopwords:\n",
    "        return getFiles(data_path + \"stem/sw/snow/snow\")\n",
    "\n",
    "    if stem == \"wn\" and not stopwords:\n",
    "        return getFiles(data_path + \"stem/original/wn/wn\")\n",
    "\n",
    "    if stem == \"wn\" and stopwords:\n",
    "        return getFiles(data_path + \"stem/sw/wn/wn\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords: False  stem: None\n",
      "Total terms: 5062336\n",
      "Unique terms: 368042\n",
      "213916, the\n",
      "117481, to\n",
      "105565, of\n",
      " 93844, a\n",
      " 87199, and\n",
      " 64854, is\n",
      " 63686, I\n",
      " 62415, in\n",
      " 59209, that\n",
      " 47701, >\n",
      " 40775, for\n",
      " 34090, you\n",
      " 32783, it\n",
      " 29996, be\n",
      " 29450, have\n",
      "stopwords: True  stem: None\n",
      "Total terms: 2931680\n",
      "Unique terms: 177684\n",
      " 15270, would\n",
      " 14409, one\n",
      " 13966, writes\n",
      " 11937, article\n",
      "  9871, dont\n",
      "  9665, like\n",
      "  9539, people\n",
      "  9226, it\n",
      "  8717, 1\n",
      "  8698, know\n",
      "  8305, get\n",
      "  7633, think\n",
      "  7323, also\n",
      "  7150, the\n",
      "  6904, 2\n",
      "stopwords: False  stem: wn\n",
      "Total terms: 4876222\n",
      "Unique terms: 176183\n",
      "242865, the\n",
      "126368, a\n",
      "122005, to\n",
      "108943, of\n",
      " 95239, and\n",
      " 81423, in\n",
      " 70648, is\n",
      " 70519, i\n",
      " 64534, that\n",
      " 53288, it\n",
      " 45073, for\n",
      " 42310, you\n",
      " 33122, this\n",
      " 32436, on\n",
      " 31297, be\n",
      "stopwords: True  stem: wn\n",
      "Total terms: 2677593\n",
      "Unique terms: 129166\n",
      " 15252, would\n",
      " 14895, one\n",
      " 10149, writes\n",
      "  9768, dont\n",
      "  9730, like\n",
      "  9579, people\n",
      "  9251, know\n",
      "  9184, it\n",
      "  9006, get\n",
      "  8317, x\n",
      "  7818, think\n",
      "  7408, time\n",
      "  7279, also\n",
      "  7243, the\n",
      "  6637, use\n",
      "stopwords: False  stem: lanc\n",
      "Total terms: 4876221\n",
      "Unique terms: 171451\n",
      "243817, the\n",
      "122005, to\n",
      "108948, of\n",
      "101442, a\n",
      " 95264, and\n",
      " 81950, in\n",
      " 70885, is\n",
      " 70521, i\n",
      " 64636, that\n",
      " 53380, it\n",
      " 53019, on\n",
      " 45283, for\n",
      " 42383, you\n",
      " 33036, not\n",
      " 31546, hav\n",
      "stopwords: True  stem: lanc\n",
      "Total terms: 2932597\n",
      "Unique terms: 171448\n",
      " 22125, us\n",
      " 15275, would\n",
      " 14678, on\n",
      " 13737, writes\n",
      " 11805, artic\n",
      " 10473, get\n",
      " 10077, lik\n",
      "  9871, dont\n",
      "  9575, know\n",
      "  9318, it\n",
      "  8717, 1\n",
      "  8600, im\n",
      "  8424, think\n",
      "  8102, the\n",
      "  8017, peopl\n",
      "stopwords: False  stem: snow\n",
      "Total terms: 4876193\n",
      "Unique terms: 171710\n",
      "242796, the\n",
      "122006, to\n",
      "108953, of\n",
      "101459, a\n",
      " 95240, and\n",
      " 81424, in\n",
      " 70645, is\n",
      " 70537, i\n",
      " 67133, that\n",
      " 59702, it\n",
      " 45078, for\n",
      " 42311, you\n",
      " 35352, be\n",
      " 33114, this\n",
      " 32676, on\n",
      "stopwords: True  stem: snow\n",
      "Total terms: 2932569\n",
      "Unique terms: 171708\n",
      " 15640, it\n",
      " 15271, would\n",
      " 15249, one\n",
      " 14506, use\n",
      " 13729, writes\n",
      " 11821, articl\n",
      " 10744, like\n",
      " 10542, get\n",
      "  9872, dont\n",
      "  9596, know\n",
      "  8721, 1\n",
      "  8435, peopl\n",
      "  8416, think\n",
      "  7561, time\n",
      "  7331, make\n",
      "stopwords: False  stem: porter\n",
      "Total terms: 4876185\n",
      "Unique terms: 172284\n",
      "242796, the\n",
      "122010, to\n",
      "108953, of\n",
      "102602, a\n",
      " 95243, and\n",
      " 81424, in\n",
      " 71512, i\n",
      " 69670, is\n",
      " 67133, that\n",
      " 59690, it\n",
      " 45078, for\n",
      " 42309, you\n",
      " 35348, be\n",
      " 32453, have\n",
      " 32440, on\n",
      "stopwords: True  stem: porter\n",
      "Total terms: 2682823\n",
      "Unique terms: 127030\n",
      " 15571, it\n",
      " 15248, would\n",
      " 15163, one\n",
      " 14468, use\n",
      " 10725, like\n",
      " 10489, get\n",
      "  9952, writes\n",
      "  9767, dont\n",
      "  9541, know\n",
      "  8385, think\n",
      "  8294, x\n",
      "  8207, peopl\n",
      "  7548, time\n",
      "  7447, i\n",
      "  7316, make\n"
     ]
    }
   ],
   "source": [
    "getTop10(getData())\n",
    "getTop10(getData(True))\n",
    "getTop10(getData(False, \"wn\"))\n",
    "getTop10(getData(True, \"wn\"))\n",
    "getTop10(getData(False, \"lanc\"))\n",
    "getTop10(getData(True, \"lanc\"))\n",
    "getTop10(getData(False, \"snow\"))\n",
    "getTop10(getData(True, \"snow\"))\n",
    "getTop10(getData(False, \"porter\"))\n",
    "getTop10(getData(True, \"porter\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = getData(True)\n",
    "\n",
    "datao = \"\"\n",
    "for d in data:\n",
    "    datao += d\n",
    "    \n",
    "datao = datao.split()\n",
    "vectorizer = TfidfVectorizer()\n",
    "tdm = vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=2, random_state=0, n_jobs=(-1)).fit(tdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(km, open(\"km.txt\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "km = pickle.load( open(\"km.txt\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for i in range(0, 1000):\n",
    "#   print(km.labels_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def getFiles(path, data_size=19393):\n",
    "    data = []\n",
    "    for i in range(0, data_size):\n",
    "        with open(path + str(i) + \".txt\", 'r') as f:\n",
    "            data.append(f.read())\n",
    "    return data\n",
    "\n",
    "def getData(stopwords=False, stem=None):\n",
    "    print(\"stopwords:\", stopwords, \" stem:\", stem)\n",
    "    data_size = 19393\n",
    "    data_path = \"data/\"\n",
    "\n",
    "    if not stopwords  and stem == None:\n",
    "        return getFiles(data_path + \"no-stem/original/original\")\n",
    "\n",
    "    if stopwords and stem == None:\n",
    "        return getFiles(data_path + \"no-stem/sw/sw\")\n",
    "\n",
    "    if stem == \"lanc\" and not stopwords:\n",
    "        return getFiles(data_path + \"stem/original/lanc/lanc\")\n",
    "\n",
    "    if stem == \"lanc\" and stopwords:\n",
    "        return getFiles(data_path + \"stem/sw/lanc/lanc\")\n",
    "\n",
    "    if stem == \"porter\" and not stopwords:\n",
    "        return getFiles(data_path + \"stem/original/porter/porter\")\n",
    "\n",
    "    if stem == \"porter\" and stopwords:\n",
    "        return getFiles(data_path + \"stem/sw/porter/porter\")\n",
    "\n",
    "    if stem == \"snow\" and not stopwords:\n",
    "        return getFiles(data_path + \"stem/original/snow/snow\")\n",
    "\n",
    "    if stem == \"snow\" and stopwords:\n",
    "        return getFiles(data_path + \"stem/sw/snow/snow\")\n",
    "\n",
    "    if stem == \"wn\" and not stopwords:\n",
    "        return getFiles(data_path + \"stem/original/wn/wn\")\n",
    "\n",
    "    if stem == \"wn\" and stopwords:\n",
    "        return getFiles(data_path + \"stem/sw/wn/wn\")\n",
    "\n",
    "def getTop10(data):\n",
    "    datao = \"\"\n",
    "    for d in data:\n",
    "        datao += d\n",
    "\n",
    "    datao = datao.split()\n",
    "    print(\"Total terms: {}\".format(len(datao)))\n",
    "    num_unique_terms = len(set(datao))\n",
    "    print(\"Unique terms: {}\".format(num_unique_terms))\n",
    "\n",
    "    text = nltk.Text(datao)\n",
    "\n",
    "    fdist1 = nltk.probability.FreqDist(text)\n",
    "    for w in fdist1.most_common(15):\n",
    "        print(\"{:>6}, {}\".format(w[1], w[0]))\n",
    "\n",
    "def printClusterTerms(km, vectorizer):\n",
    "    assigned_cluster = km.labels_\n",
    "    cluster_terms = []\n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(km.n_clusters):\n",
    "        print(\"Cluster {:>2} ({:>4} docs): \".format(i, len([x for x in km.labels_ if x == i])), end='')\n",
    "        cterms = []\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' {}'.format(terms[ind]), end='')\n",
    "            cterms.append(terms[ind])\n",
    "        cluster_terms.append(' '.join(cterms))\n",
    "        print(\"\")\n",
    "        \n",
    "def trainCustomSets(data, k):\n",
    "    global km\n",
    "    print(\"Number of clusters:\", k)\n",
    "    getTop10(data)\n",
    "    vectorizer = TfidfVectorizer(min_df=2, max_df = 800)\n",
    "    tdm = vectorizer.fit_transform(data)\n",
    "    km = KMeans(n_clusters=k, random_state=0, n_jobs=(-1)).fit(tdm)\n",
    "    scr = km.score(tdm)\n",
    "    #calinski_scr = metrics.calinski_harabaz_score(tdm.toarray(), km.labels_)\n",
    "    #silhoette_scr = metrics.silhouette_score(tdm, km.labels_ , metric='euclidean',sample_size=8000)\n",
    "    print(\"scr: \", scr)\n",
    "    printClusterTerms(km, vectorizer)\n",
    "    \n",
    "trainCustomSets(getData(True, \"wn\"), 2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords: True  stem: wn\n",
      "Number of clusters: 2\n",
      "Total terms: 2677593\n",
      "Unique terms: 129166\n",
      " 15252, would\n",
      " 14895, one\n",
      " 10149, writes\n",
      "  9768, dont\n",
      "  9730, like\n",
      "  9579, people\n",
      "  9251, know\n",
      "  9184, it\n",
      "  9006, get\n",
      "  8317, x\n",
      "  7818, think\n",
      "  7408, time\n",
      "  7279, also\n",
      "  7243, the\n",
      "  6637, use\n",
      "scr:  -19195.959871224622\n",
      "Cluster  0 (4159 docs):  jesus gun religion israel bible fbi church child fire jew\n",
      "Cluster  1 (15234 docs):  chip disk driver mac monitor mb bike pc sale speed\n"
     ]
    }
   ],
   "source": [
    "trainCustomSets(getData(True, \"wn\"), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "calinski_scr = metrics.calinski_harabaz_score(tdm.toarray(), km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def getFiles(path, data_size=19393):\n",
    "    data = []\n",
    "    for i in range(0, data_size):\n",
    "        with open(path + str(i) + \".txt\", 'r') as f:\n",
    "            data.append(f.read())\n",
    "    return data\n",
    "\n",
    "def getData(stopwords=False, stem=None):\n",
    "    print(\"stopwords:\", stopwords, \" stem:\", stem)\n",
    "    data_size = 19393\n",
    "    data_path = \"data/\"\n",
    "\n",
    "    if not stopwords  and stem == None:\n",
    "        return getFiles(data_path + \"no-stem/original/original\")\n",
    "\n",
    "    if stopwords and stem == None:\n",
    "        return getFiles(data_path + \"no-stem/sw/sw\")\n",
    "\n",
    "    if stem == \"lanc\" and not stopwords:\n",
    "        return getFiles(data_path + \"stem/original/lanc/lanc\")\n",
    "\n",
    "    if stem == \"lanc\" and stopwords:\n",
    "        return getFiles(data_path + \"stem/sw/lanc/lanc\")\n",
    "\n",
    "    if stem == \"porter\" and not stopwords:\n",
    "        return getFiles(data_path + \"stem/original/porter/porter\")\n",
    "\n",
    "    if stem == \"porter\" and stopwords:\n",
    "        return getFiles(data_path + \"stem/sw/porter/porter\")\n",
    "\n",
    "    if stem == \"snow\" and not stopwords:\n",
    "        return getFiles(data_path + \"stem/original/snow/snow\")\n",
    "\n",
    "    if stem == \"snow\" and stopwords:\n",
    "        return getFiles(data_path + \"stem/sw/snow/snow\")\n",
    "\n",
    "    if stem == \"wn\" and not stopwords:\n",
    "        return getFiles(data_path + \"stem/original/wn/wn\")\n",
    "\n",
    "    if stem == \"wn\" and stopwords:\n",
    "        return getFiles(data_path + \"stem/sw/wn/wn\")\n",
    "\n",
    "def printClusterTerms(km, vectorizer):\n",
    "    assigned_cluster = km.labels_\n",
    "    cluster_terms = []\n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(km.n_clusters):\n",
    "        print(\"Cluster {:>2} ({:>4} docs): \".format(i, len([x for x in km.labels_ if x == i])), end='')\n",
    "        cterms = []\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' {}'.format(terms[ind]), end='')\n",
    "            cterms.append(terms[ind])\n",
    "        cluster_terms.append(' '.join(cterms))\n",
    "        print(\"\")\n",
    "        \n",
    "def trainCustomSets(data, k):\n",
    "    global km\n",
    "    print(\"Number of clusters:\", k)\n",
    "    #getTop10(data)\n",
    "    vectorizer = TfidfVectorizer(min_df=2, max_df = 800)\n",
    "    tdm = vectorizer.fit_transform(data)\n",
    "    km = KMeans(n_clusters=k, random_state=0, n_jobs=(-1)).fit(tdm)\n",
    "    scr = km.score(tdm)\n",
    "    #calinski_scr = metrics.calinski_harabaz_score(tdm.toarray(), km.labels_)\n",
    "    #silhoette_scr = metrics.silhouette_score(tdm, km.labels_ , metric='euclidean',sample_size=8000)\n",
    "    print(\"scr: \", scr)\n",
    "    printClusterTerms(km, vectorizer)\n",
    "    return scr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords: True  stem: wn\n",
      "Number of clusters: 47\n"
     ]
    }
   ],
   "source": [
    "file = open(\"kmeansCustomSet.txt\", \"w\")\n",
    "scr_vector = []\n",
    "calinski_scr_vector =[]\n",
    "silhoette_scr_vector = []\n",
    "for x in [47, 80]:\n",
    "    for y in [\"wn\", \"lanc\", \"snow\", \"porter\"]:\n",
    "        scr = trainCustomSets(getData(True, y), x) \n",
    "\n",
    "        scr_vector.append(scr)\n",
    "\n",
    "        file.write(\"Kmeans: NÂ°: \"+ str(x) +\" - score= \"+ str(scr))\n",
    "        file.flush()\n",
    "\n",
    "file.write(\"\\n\\nscore: \" + str(scr_vector))\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
