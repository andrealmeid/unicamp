{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Bibliotecas utilizadas ###\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from numpy import genfromtxt\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Tratamento de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilizando a base `data.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Pegando dados do banco de dados (data.csv) ###\n",
    "database = genfromtxt('Database/data.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funções de pré-processamento para tratamentos alternativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFiles(path, data_size=19393):\n",
    "    data = []\n",
    "    for i in range(0, data_size):\n",
    "        with open(path + str(i) + \".txt\", 'r') as f:\n",
    "            data.append(f.read())\n",
    "    return data\n",
    "\n",
    "def getData(stopwords=False, stem=None):\n",
    "    print(\"stopwords:\", stopwords, \" stem:\", stem)\n",
    "    data_size = 19393\n",
    "    data_path = \"data/\"\n",
    "\n",
    "    if not stopwords  and stem == None:\n",
    "        return getFiles(data_path + \"no-stem/original/original\")\n",
    "\n",
    "    if stopwords and stem == None:\n",
    "        return getFiles(data_path + \"no-stem/sw/sw\")\n",
    "\n",
    "    if stem == \"lanc\" and not stopwords:\n",
    "        return getFiles(data_path + \"stem/original/lanc/lanc\")\n",
    "\n",
    "    if stem == \"lanc\" and stopwords:\n",
    "        return getFiles(data_path + \"stem/sw/lanc/lanc\")\n",
    "\n",
    "    if stem == \"porter\" and not stopwords:\n",
    "        return getFiles(data_path + \"stem/original/porter/porter\")\n",
    "\n",
    "    if stem == \"porter\" and stopwords:\n",
    "        return getFiles(data_path + \"stem/sw/porter/porter\")\n",
    "\n",
    "    if stem == \"snow\" and not stopwords:\n",
    "        return getFiles(data_path + \"stem/original/snow/snow\")\n",
    "\n",
    "    if stem == \"snow\" and stopwords:\n",
    "        return getFiles(data_path + \"stem/sw/snow/snow\")\n",
    "\n",
    "    if stem == \"wn\" and not stopwords:\n",
    "        return getFiles(data_path + \"stem/original/wn/wn\")\n",
    "\n",
    "    if stem == \"wn\" and stopwords:\n",
    "        return getFiles(data_path + \"stem/sw/wn/wn\")\n",
    "\n",
    "def printClusterTerms(km, vectorizer):\n",
    "    assigned_cluster = km.labels_\n",
    "    cluster_terms = []\n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(km.n_clusters):\n",
    "        print(\"Cluster {:>2} ({:>4} docs): \".format(i, len([x for x in km.labels_ if x == i])), end='')\n",
    "        cterms = []\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' {}'.format(terms[ind]), end='')\n",
    "            cterms.append(terms[ind])\n",
    "        cluster_terms.append(' '.join(cterms))\n",
    "        print(\"\")\n",
    "        \n",
    "def trainCustomSets(data, k):\n",
    "    global km\n",
    "    print(\"Number of clusters:\", k)\n",
    "    #getTop10(data)\n",
    "    vectorizer = TfidfVectorizer(min_df=2, max_df = 800)\n",
    "    tdm = vectorizer.fit_transform(data)\n",
    "    km = KMeans(n_clusters=k, random_state=0, n_jobs=(-1)).fit(tdm)\n",
    "    scr = km.score(tdm)\n",
    "    #calinski_scr = metrics.calinski_harabaz_score(tdm.toarray(), km.labels_)\n",
    "    #silhoette_scr = metrics.silhouette_score(tdm, km.labels_ , metric='euclidean',sample_size=8000)\n",
    "    print(\"scr: \", scr)\n",
    "    printClusterTerms(km, vectorizer)\n",
    "    return scr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open(\"Results/kmeansCustomSet.txt\", \"w\")\n",
    "scr_vector = []\n",
    "calinski_scr_vector =[]\n",
    "silhoette_scr_vector = []\n",
    "for x in [47, 80]:\n",
    "    for y in [\"wn\", \"lanc\", \"snow\", \"porter\"]:\n",
    "        scr = trainCustomSets(getData(True, y), x) \n",
    "\n",
    "        scr_vector.append(scr)\n",
    "\n",
    "        file.write(\"Kmeans: N°: \"+ str(x) +\" - score= \"+ str(scr))\n",
    "        file.flush()\n",
    "\n",
    "file.write(\"\\n\\nscore: \" + str(scr_vector))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# recebe a porcentagem do PCA e a base de dados. Retorna a base com PCA\n",
    "def applyPCA(perc, database):\n",
    "    pca = PCA(n_components=perc)\n",
    "    pca.fit(database)\n",
    "    pca_fit = pca.transform(database)\n",
    "\n",
    "    soma = 0\n",
    "    for i in pca.explained_variance_ratio_:\n",
    "        soma += i\n",
    "    print(\"Dados removidos/reduzidos: \", soma)\n",
    "    \n",
    "    return pca_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste sem pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tic = time.clock()\n",
    "\n",
    "km = KMeans(n_clusters=20, random_state=0, n_jobs=(-1)).fit(database)\n",
    "print(km.inertia_)\n",
    "scr = km.score(database)\n",
    "print(scr)\n",
    "\n",
    "calinski_scr = metrics.calinski_harabaz_score(database, km.labels_)\n",
    "print(calinski_scr)\n",
    "\n",
    "print(\"Tempo sem PCA: \",time.clock() - tic,\"s\")\n",
    "\n",
    "tic = time.clock()\n",
    "silhoette_scr = metrics.silhouette_score(database, km.labels_ , metric='euclidean',sample_size=8000)\n",
    "print(silhoette_scr)\n",
    "print(\"Tempo: \",time.clock() - tic,\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste com PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Aplica PCA ##\n",
    "pca_db = applyPCA(0.85, database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tic = time.clock()\n",
    "\n",
    "km = KMeans(n_clusters=20, random_state=0, n_jobs=(-1)).fit(pca_db)\n",
    "scr = km.score(pca_db)\n",
    "print(scr)\n",
    "\n",
    "calinski_scr = metrics.calinski_harabaz_score(pca_db, km.labels_)\n",
    "print(calinski_scr)\n",
    "\n",
    "print(\"Tempo sem PCA: \",time.clock() - tic,\"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação sem pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open(\"Results/kmeansSemPCA.txt\", \"w\")\n",
    "scr_vector = []\n",
    "calinski_scr_vector =[]\n",
    "silhoette_scr_vector = []\n",
    "for x in range(2,201):\n",
    "    \n",
    "    km = KMeans(n_clusters=x, random_state=0, n_jobs=(-1)).fit(database)\n",
    "    scr = km.score(database)\n",
    "    calinski_scr = metrics.calinski_harabaz_score(database, km.labels_)\n",
    "    silhoette_scr = metrics.silhouette_score(database, km.labels_ , metric='euclidean',sample_size=8000)\n",
    "    \n",
    "    scr_vector.append(scr)\n",
    "    calinski_scr_vector.append(calinski_scr)\n",
    "    silhoette_scr_vector.append(silhoette_scr)\n",
    "    print(str(scr), \"  \",str(calinski_scr), \"  \", str(silhoette_scr))\n",
    "    \n",
    "    file.write(\"Kmeans: N°: \"+ str(x) +\" - score= \"+ str(scr) +\" calinski= \"+ str(calinski_scr)+\" silhoette=\"+str(silhoette_scr)+\"\\n\")    \n",
    "    file.flush()\n",
    "\n",
    "file.write(\"\\n\\nscore: \" + str(scr_vector))\n",
    "file.write(\"\\n\\ncalinski: \" + str(calinski_scr_vector))\n",
    "file.write(\"\\n\\nsilhoette: \" + str(silhoette_scr_vector))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=x, random_state=0, n_jobs=(-1)).fit(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráficos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gráfico de score vs Nº cluster\n",
    "plt.plot(range(2,201), scr_vector)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Nº clusters')\n",
    "plt.title('K-Means score vs Nº cluster')\n",
    "plt.show()\n",
    "\n",
    "# Gráfico de calinski score vs Nº cluster\n",
    "plt.plot(range(2,201), calinski_scr_vector)\n",
    "plt.ylabel('calinski score')\n",
    "plt.xlabel('Nº clusters')\n",
    "plt.title('Calinski score vs Nº cluster')\n",
    "plt.show()\n",
    "\n",
    "# Gráfico de silhoette score vs Nº cluster\n",
    "plt.plot(range(2,201), silhoette_scr_vector)\n",
    "plt.ylabel('silhoette score')\n",
    "plt.xlabel('Nº clusters')\n",
    "plt.title('silhoette score vs Nº cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação com pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open(\"Results/kmeansComPCA.txt\", \"w\")\n",
    "\n",
    "matrix_calinski_scr = matrix_scr = matrix_silhoette_scr = [[0]*199 for i in range(5)]\n",
    "pca_values = [0.85, 0.90, 0.93, 0.95, 0.97]\n",
    "for x in pca_values:\n",
    "    pca_db = applyPCA(x, database)\n",
    "    for y in range(2,201):\n",
    "        km = KMeans(n_clusters=y, random_state=0, n_jobs=(-1)).fit(pca_db)\n",
    "        scr = km.score(pca_db)\n",
    "        calinski_scr = metrics.calinski_harabaz_score(pca_db, km.labels_)\n",
    "        silhoette_scr = metrics.silhouette_score(pca_db, km.labels_ , metric='euclidean',sample_size=8000)\n",
    "        \n",
    "        matrix_scr[pca_values.index(x)][y-2] = scr\n",
    "        matrix_calinski_scr[pca_values.index(x)][y-2] = calinski_scr\n",
    "        matrix_silhoette_scr[pca_values.index(x)][y-2] = silhoette_scr\n",
    "        print(str(scr), \"  \",str(calinski_scr), \"  \", str(silhoette_scr))\n",
    "        \n",
    "        file.write(\"Kmeans: PCA:\"+ str(x) +\" N°:\"+ str(y)+\"  - score= \"+ str(scr) +\" calinski= \"+ str(calinski_scr)+\" silhoette=\"+str(silhoette_scr)+\"\\n\")    \n",
    "        file.flush()\n",
    " \n",
    "file.write(\"\\n\\nscore: \" + str(matrix_scr))\n",
    "file.write(\"\\n\\ncalinski: \" + str(matrix_calinski_scr))\n",
    "file.write(\"\\n\\nsilhoette: \" + str(matrix_silhoette_scr))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráficos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gráfico de score vs Nº cluster\n",
    "plt.plot(range(2,201), matrix_scr[0])\n",
    "plt.plot(range(2,201), matrix_scr[1])\n",
    "plt.plot(range(2,201), matrix_scr[2])\n",
    "plt.plot(range(2,201), matrix_scr[3])\n",
    "plt.plot(range(2,201), matrix_scr[4])\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Nº clusters')\n",
    "plt.legend(['PCA = 0.85', 'PCA = 0.90', 'PCA = 0.93', 'PCA = 0.95', 'PCA = 0.97' ], loc=4)\n",
    "plt.title('K-Means score vs Nº cluster')\n",
    "plt.show()\n",
    "\n",
    "# Gráfico de calinski score vs Nº cluster\n",
    "plt.plot(range(2,201), matrix_calinski_scr[0])\n",
    "plt.plot(range(2,201), matrix_calinski_scr[1])\n",
    "plt.plot(range(2,201), matrix_calinski_scr[2])\n",
    "plt.plot(range(2,201), matrix_calinski_scr[3])\n",
    "plt.plot(range(2,201), matrix_calinski_scr[4])\n",
    "plt.ylabel('calinski score')\n",
    "plt.xlabel('Nº clusters')\n",
    "plt.legend(['PCA = 0.85', 'PCA = 0.90', 'PCA = 0.93', 'PCA = 0.95', 'PCA = 0.97' ], loc=4)\n",
    "plt.title('Calinski score vs Nº cluster')\n",
    "plt.show()\n",
    "\n",
    "# Gráfico de silhoette score vs Nº cluster\n",
    "plt.plot(range(2,201), matrix_silhoette_scr[0])\n",
    "plt.plot(range(2,201), matrix_silhoette_scr[1])\n",
    "plt.plot(range(2,201), matrix_silhoette_scr[2])\n",
    "plt.plot(range(2,201), matrix_silhoette_scr[3])\n",
    "plt.plot(range(2,201), matrix_silhoette_scr[4])\n",
    "plt.ylabel('silhoette score')\n",
    "plt.xlabel('Nº clusters')\n",
    "plt.legend(['PCA = 0.85', 'PCA = 0.90', 'PCA = 0.93', 'PCA = 0.95', 'PCA = 0.97' ], loc=4)\n",
    "plt.title('silhoette score vs Nº cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação das prováveis quantidade de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotclusters (km, clusters):\n",
    "    # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "    h = .001 # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    x_min, x_max = reduced_data[:, 0].min() - 1, reduced_data[:, 0].max() + 1\n",
    "    y_min, y_max = reduced_data[:, 1].min() - 1, reduced_data[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Obtain labels for each point in mesh. Use last trained model.\n",
    "    Z = km.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    plt.imshow(Z, interpolation='nearest',\n",
    "               extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "               cmap=plt.cm.Paired,\n",
    "               aspect='auto', origin='lower')\n",
    "\n",
    "    plt.plot(reduced_data[:, 0], reduced_data[:, 1], 'k.', markersize=2)\n",
    "    # Plot the centroids as a white X\n",
    "    centroids = km.cluster_centers_\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "                marker='.', s=10, linewidths=3,\n",
    "                color='w', zorder=10)\n",
    "    plt.title('K-means clustering with '+ str(clusters)+ ' clusters\\nCentroids are marked with white dots')\n",
    "    plt.xlim(x_min+0.85, x_max - 0.9)\n",
    "    plt.ylim(y_min+0.9, y_max - 0.85)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Função para obter os medoides e os pontos mais proximos aos medoids\n",
    "def getMedoid(allset, centroid):\n",
    "    mindist = float('inf')\n",
    "    allset = np.array(allset)\n",
    "    indice = 0\n",
    "    for i in allset:\n",
    "        curdist = cdist(np.atleast_2d(centroid), np.atleast_2d(i))[0][0]\n",
    "        if curdist < mindist and curdist != 0:\n",
    "            mindist = curdist\n",
    "            medoid = i\n",
    "            mindice = indice\n",
    "        indice = indice +1\n",
    "    return medoid, mindice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=47, random_state=0, n_jobs=(-1)).fit(database)\n",
    "km2 = KMeans(n_clusters=80, random_state=0, n_jobs=(-1)).fit(database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open(\"Results/Medoids47Clusters.txt\", \"w\")\n",
    "file2 = open(\"Results/MaisProximos47Clusters.txt\", \"w\")\n",
    "\n",
    "file2.write(\"medoid_id   closest_id\\n\")\n",
    "for x in km.cluster_centers_ :\n",
    "    medoid_position, medoid_id = getMedoid(database,x)\n",
    "    closest_position, closest_id = getMedoid(database,medoid_position)\n",
    "    file.write(str(medoid_position)+\"\\n\")\n",
    "    file2.write(str(medoid_id)+\" , \"+str(closest_id)+\"\\n\")\n",
    "\n",
    "file.close()\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open(\"Results/Medoids80Clusters.txt\", \"w\")\n",
    "file2 = open(\"Results/MaisProximos80Clusters.txt\", \"w\")\n",
    "\n",
    "medoids = closest_text = medoids2 = closest_text2 = []\n",
    "for x in km2.cluster_centers_ :\n",
    "    currmedois, closest = getMedoid(database,x)\n",
    "    file.write(str(currmedois)+\"\\n\\n\")\n",
    "    file2.write(str(closest)+\"\\n\")\n",
    "\n",
    "file.close()\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Redução da dimensionalidade do vetor de variáveis e fit com a quantidade de clusters escolhida\n",
    "reduced_data = PCA(n_components=2).fit_transform(database)\n",
    "km = KMeans(n_clusters=47, random_state=0, n_jobs=(-1)).fit(reduced_data)\n",
    "km2 = KMeans(n_clusters=80, random_state=0, n_jobs=(-1)).fit(reduced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot dos diagramas de Voronoi para 47 e 80 clusters\n",
    "plotclusters(km, 47)\n",
    "plotclusters(km2, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
